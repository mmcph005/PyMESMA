{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook imports and refomats environmental data for timeseries stats analysis.\n",
    "Meredith L. McPherson, University of California - Santa Cruz\n",
    "Updated 1/25/2021\n",
    "***********************************************************************************************************************************\n",
    "\n",
    "PDO INDEX\n",
    "\n",
    "If the columns of the table appear without formatting on your browser, use https://oceanview.pfeg.noaa.gov/erddap/tabledap/cciea_OC_PDO.htmlTable?time,PDO\n",
    "\n",
    "Updated standardized values for the PDO index, derived as the \n",
    "leading PC of monthly SST anomalies in the North Pacific Ocean, \n",
    "poleward of 20N. The monthly mean global average SST anomalies\n",
    "are removed to separate this pattern of variability from any \n",
    "\"global warming\" signal that may be present in the data. \n",
    "\n",
    "\n",
    "For more details, see:\n",
    "\n",
    " Zhang, Y., J.M. Wallace, D.S. Battisti, 1997: \n",
    "     ENSO-like interdecadal variability: 1900-93. J. Climate, 10, 1004-1020. \n",
    "\n",
    " Mantua, N.J. and S.R. Hare, Y. Zhang, J.M. Wallace, and R.C. Francis,1997: \n",
    "     A Pacific interdecadal climate oscillation with impacts on salmon \n",
    "     production. Bulletin of the American Meteorological Society, 78, \n",
    "     pp. 1069-1079.\n",
    "\n",
    "\n",
    "Data sources for this index are: \n",
    " UKMO Historical SST data set for 1900-81; \n",
    " Reynold's Optimally Interpolated SST (V1) for January 1982-Dec 2001)\n",
    "*** OI SST Version 2 (V2) beginning January 2002 -  \n",
    "\n",
    "** 2002-2018 Derived from OI.v2 SST fields\n",
    "A graphic comparing monthly PDO values for 1982-2002 derived from the v1 and v2 \n",
    "sst products is available at \n",
    "http://jisao.washington.edu/pdo/img/v1v2PDOComp.png\n",
    "\n",
    "If you have any questions about this time series, contact\n",
    "Nathan Mantua at: nate.mantua@noaa.gov\n",
    "\n",
    "This file is /home/disk/margaret/jisao/pdo/PDO.latest.txt \n",
    "\n",
    "***********************************************************************************************************************************\n",
    "\n",
    "NPGO index\n",
    "\n",
    "WARNING: Values after Dec-2004 are updated  \n",
    "using Satellite SSHa from AVISO Delayed Time product.   \n",
    "http://www.o3d.org/npgo/npgo.php\n",
    "\n",
    "The update is performed by taking the NPGO spatial pattern of Di Lorenzo et al. 2008 \n",
    "computed over the period 1950-2004, and projecting the AVISO Satellite SSHa. \n",
    "During the pre-processing of the AVISO data, we remove the seasonal cycle based on \n",
    "the 1993-2004 seasonal means. \n",
    " \n",
    "AVISO PRODUCT UPDATE Summer 2014: AVISO has released a re-processed dataset for the sea level. \n",
    "Starting from the November 2014, the NPGO index is computed with this updated dataset. NPGO \n",
    "values from 2004 onward have been recomputed with very minor differences from previous releases. \n",
    "\n",
    "Ref: \n",
    "Di Lorenzo et al., 2008: North Pacific Gyre Oscillation  \n",
    "links ocean climate and ecosystem change, GRL. \n",
    "\n",
    "***********************************************************************************************************************************\n",
    "\n",
    "MEI Index\n",
    "https://psl.noaa.gov/enso/mei/data/meiv2.data\n",
    "\n",
    "A new version of the MEI (MEI.v2) has been created that uses 5 variables (sea level pressure (SLP), sea surface temperature (SST), surface zonal winds (U), surface meridional winds (V), and Outgoing Longwave Radiation (OLR)) to produce a time series of ENSO conditions from 1979 to present. The MEI.v2 expands upon the original MEI developed by Wolter and Timlin (1993) which was calculated using 6 variables as proxies for ENSO relevant atmosphere and ocean conditions.\n",
    "In MEI.v2, the fields of SST, SLP, and surface zonal and meridional winds are obtained from the high-quality JRA-55 global reanalysis (Kobayashi et al. 2015). In contrast, the original MEI (Wolter and Timlin, 1993) used marine ship observations based on the International Comprehensive Ocean Atmosphere Data Set (ICOADS) and used near-surface air temperature as well as SST. The MEI.v2 also uses observations of OLR from NOAA Climate Data Record (CDR) of Monthly Outgoing Longwave Radiation (OLR), Version 2.2-1 (available from NOAA National Centers for Environmental Information (NCEI)); whereas the original MEI used ICOADS cloud cover fraction data. To produce the MEI.v2, all variables are interpolated to a common 2.5° latitude-longitude grid and standardized anomalies are computed with respect to the reference period of 1980-2018. As with the original version of the MEI (Wolter and Timlin, 2011), the MEI.v2 is calculated as the leading principal component (PC) time series of the Empirical Orthogonal Function (EOF) of the standardized anomalies of the above 5 combined variables over the tropical Pacific during 1980-2018. The EOF analysis is based on the covariance matrix and the analysis domain is the same as for the original MEI (30°S-30°N and 100°E-70°W, excluding the Atlantic Ocean and the land regions). A latitudinal weighting prior to the EOF analysis is applied.\n",
    "The EOF analysis for MEI.v2 is conducted for 12 partially overlapping 2-month \"seasons\" (e.g., Wolter and Timlin, 1993). To obtain MEI.v2 values before 1980 and after 2018, standardized anomalies maps relative to the 1980-2018 reference period are projected onto the leading EOF pattern. Because the OLR record starts in January 1979, the DJ 1979 MEI.v2 value is based on January 1979 OLR data only.\n",
    "\n",
    "************************************************************************************************************************************\n",
    "AVHRR SST\n",
    "https://www.ncei.noaa.gov/erddap/griddap/ncdc_oisst_v2_avhrr_by_time_zlev_lat_lon.html\n",
    "Dataset Title: \tOISST-V2-AVHRR Daily 1/4 degree By time, depth, latitude, longitude   RSS\n",
    "Institution: \tNOAA/NCEI   (Dataset ID: ncdc_oisst_v2_avhrr_by_time_zlev_lat_lon)\n",
    "\n",
    "37.875,41.125\n",
    "302,305.875\n",
    "\n",
    "netcdf file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- import python modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os as os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import scipy.io as sio\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "from scipy.signal import find_peaks\n",
    "import datetime\n",
    "import xlrd\n",
    "from netCDF4 import Dataset\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# current date and time  \n",
    "now = datetime.datetime.now().strftime('%Y%m%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- PDO \n",
    "\n",
    "# text file format columnwise: YEAR, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, DEC\n",
    "var = 'PDO'\n",
    "filename = 'PDO_INDEX.txt'\n",
    "PDO = np.loadtxt(filename, skiprows = 41)\n",
    "PDO_max = np.zeros((len(PDO),2))\n",
    "PDO_clim = np.zeros((PDO.shape[1]-1,1))\n",
    "months = np.arange(1,13,1).reshape(12,1)\n",
    "\n",
    "# calculate climatology of PDO \n",
    "\n",
    "for w in range(PDO.shape[1]-1):\n",
    "    PDO_clim[w,0] = np.nanmean(PDO[:,w+1])\n",
    "\n",
    "# calculate the max PDO value from year\n",
    "\n",
    "for i,val in enumerate(PDO):\n",
    "    PDO_max[i,0] = int(val[0])\n",
    "    PDO_remclim = val[1:].reshape(12,1) - PDO_clim\n",
    "    max = np.nanmax(PDO_remclim)\n",
    "    PDO_max[i,1] = max\n",
    "    min = np.nanmin(PDO_remclim)\n",
    "    if np.absolute(min) > max:    \n",
    "        PDO_max[i,1] = min\n",
    "    else:\n",
    "        PDO_max[i,1] = max\n",
    "\n",
    "PDO_LS = np.array((PDO_max[np.where(PDO_max[:,0]==1985.)[0][0]:,0],PDO_max[np.where(PDO_max[:,0]==1985.)[0][0]:,1])).T\n",
    "PDO_stand = np.array((PDO_LS[:,0],sp.stats.zscore(PDO_LS[:,1]))).T\n",
    "\n",
    "save_filename = ''\n",
    "np.savetxt(save_filename,\n",
    "          PDO_stand,fmt = '%i %0.5f',\n",
    "           header = f'{var} - climatology removed and index standardized {now} \\n Year; index value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- NPGO\n",
    "\n",
    "# text file format columnwise: YEAR, MONTH, NPGO index \n",
    "var = 'NPGO'\n",
    "filename = 'NPGO_index.txt'\n",
    "NPGO = np.loadtxt(filename, skiprows = 26)\n",
    "NPGO_max = np.zeros((int((len(NPGO)/12)+1),2))\n",
    "NPGO_clim = np.zeros((12,1))\n",
    "NPGO_remclim = np.zeros((int(len(NPGO)),1))\n",
    "months = np.arange(1,13,1).reshape(12,1)\n",
    "\n",
    "# calculate climatology of NPGO\n",
    "\n",
    "for m, mon in enumerate(months):\n",
    "    index = np.where(NPGO[:,1]==mon)\n",
    "    NPGO_clim[m,0] = np.nanmean(NPGO[index,2])\n",
    "    \n",
    "for m,mon in enumerate(months):\n",
    "    mindex = np.where(NPGO[:,1]==mon)\n",
    "    NPGO_remclim[mindex,0] = NPGO[mindex,2] - NPGO_clim[m]\n",
    "\n",
    "#calculate the max NPGO value from year\n",
    "\n",
    "for k, year in enumerate(np.arange(np.min(NPGO[:,0]),np.max(NPGO[:,0])+1,1)):\n",
    "    #print(year)\n",
    "    NPGO_max[k,0] = year\n",
    "    yindex = np.where(NPGO[:,0]==year) \n",
    "    max = np.max(NPGO_remclim[yindex[0],0])\n",
    "    min = np.min(NPGO_remclim[yindex[0],0])\n",
    "    if np.absolute(min) > max:    \n",
    "        NPGO_max[k,1] = min\n",
    "    else:\n",
    "        NPGO_max[k,1] = max\n",
    "        \n",
    "NPGO_LS = np.array((NPGO_max[np.where(NPGO_max[:,0]==1985.)[0][0]:,0],NPGO_max[np.where(NPGO_max[:,0]==1985.)[0][0]:,1])).T\n",
    "NPGO_stand = np.array((NPGO_LS[:,0],sp.stats.zscore(NPGO_LS[:,1]))).T\n",
    "\n",
    "save_filename = ''\n",
    "np.savetxt(save_filename.\n",
    "           format(var),\n",
    "          NPGO_stand,fmt = '%i %0.5f',\n",
    "           header = '{0} - climatology removed and index standardized {1} \\n Year; index value'.\n",
    "           format(var,now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- MEI\n",
    "\n",
    "# text file format columnwise: YEAR, DECJAN, JANFEB, FEBMAR, MARAPR, APRMAY, MAYJUN, JUNJUL, JULAUG, AUGSEP, SEPOCT, OCTNOV, NOVDEC\n",
    "var = 'MEI'\n",
    "filename = 'MEI_Index_v2'\n",
    "MEI = np.loadtxt(filename, skiprows = 9)\n",
    "MEI_max = np.zeros((len(MEI),2))\n",
    "MEI_clim = np.zeros((MEI.shape[1]-1,1))\n",
    "months = np.arange(1,13,1).reshape(12,1)\n",
    "\n",
    "# calculate climatology of MEI\n",
    "\n",
    "for w in range(MEI.shape[1]-1):\n",
    "    MEI_clim[w,0] = np.nanmean(MEI[:,w+1])\n",
    "\n",
    "#calculate the max MEI value from year\n",
    "\n",
    "for i,val in enumerate(MEI):\n",
    "    MEI_max[i,0] = int(val[0])\n",
    "    MEI_remclim = val[1:].reshape(12,1) - MEI_clim\n",
    "    max = np.nanmax(MEI_remclim)\n",
    "    min = np.nanmin(MEI_remclim)\n",
    "    if np.absolute(min) > max:    \n",
    "        MEI_max[i,1] = min\n",
    "    else:\n",
    "        MEI_max[i,1] = max\n",
    "    \n",
    "MEI_LS = np.array((MEI_max[np.where(MEI_max[:,0]==1985.)[0][0]:,0],MEI_max[np.where(MEI_max[:,0]==1985.)[0][0]:,1])).T\n",
    "MEI_stand = np.array((MEI_LS[:,0],sp.stats.zscore(MEI_LS[:,1]))).T\n",
    "\n",
    "np.savetxt('/Volumes/Mere/Python_MESMA/Statistics/Environmental_data/Standardized_files/{0}_stand.txt'.\n",
    "           format(var),\n",
    "          MEI_stand,fmt = '%i %0.5f',\n",
    "           header = '{0} - climatology removed and index standardized {1} \\n Year; index value'.\n",
    "           format(var,now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Process AVHRR SST data: daily timeseries remove climatology\n",
    "var = 'SST_AVHRR'\n",
    "\n",
    "# ----- import each netcdf file\n",
    "filelist = glob.glob('AVHRR_SST/*.nc')\n",
    "filelist.sort()\n",
    "\n",
    "# ----- create empty array to fill with mean daily sst from region\n",
    "\n",
    "sst_dailymean = pd.DataFrame(columns = ['Timestamp',\n",
    "                                        'Date',\n",
    "                                        'Year',\n",
    "                                        'Month',\n",
    "                                        'Day',\n",
    "                                        'DOY',\n",
    "                                        'SST',\n",
    "                                       'SST-clim',\n",
    "                                        'SST-clim standardized',\n",
    "                                        'NO3'])\n",
    "\n",
    "for file in filelist:\n",
    "    #print(file)\n",
    "    nc_f = file  \n",
    "    nc_fid = Dataset(nc_f, 'r') \n",
    "    \n",
    "    lons = nc_fid.variables['longitude'][:]\n",
    "    lats = nc_fid.variables['latitude'][:]\n",
    "    time = nc_fid.variables['time'][:]\n",
    "    sst = nc_fid.variables['sst'][:]\n",
    "    \n",
    "    #print('lon range:', np.min(lons),'-',np.max(lons))\n",
    "    #print('lon range:', np.min(lats),'-',np.max(lats))\n",
    "\n",
    "    # nans are the landmask\n",
    "    for d in range(sst.shape[0]): \n",
    "        sst_dailymean = sst_dailymean.append({'SST': np.nanmean(sst[d,:,:,:]),'Timestamp': time[d]}, ignore_index=True)\n",
    "        \n",
    "# start date = 1985-01-01\n",
    "# end date = 2019-11-09\n",
    "sst_dailymean = sst_dailymean.replace({0:np.nan})\n",
    "sst_dailymean['Timestamp'] = sst_dailymean['Timestamp'].astype(int)\n",
    "sst_dailymean['Date'] = pd.to_datetime(sst_dailymean['Timestamp'], unit = 's')\n",
    "sst_dailymean['Year'] = pd.DatetimeIndex(sst_dailymean['Date']).year\n",
    "sst_dailymean['Month'] = pd.DatetimeIndex(sst_dailymean['Date']).month\n",
    "sst_dailymean['Day'] = pd.DatetimeIndex(sst_dailymean['Date']).day\n",
    "sst_dailymean['DOY'] = pd.DatetimeIndex(sst_dailymean['Date']).dayofyear\n",
    "\n",
    "# ----- calculate NO3 concentration based on daily sst\n",
    "sst_dailymean.loc[sst_dailymean.SST > 13.1,'NO3'] = 0\n",
    "sst_dailymean.loc[sst_dailymean.SST <= 13.1,'NO3'] = 86.2 - (sst_dailymean['SST']* 6.6)\n",
    "\n",
    "sst_clim = pd.DataFrame()\n",
    "\n",
    "sst_clim['SST-DOY'] = sst_dailymean.groupby(sst_dailymean['DOY'])['SST'].mean()\n",
    "#sst_clim['SST-Month'] = sst_dailymean.groupby(sst_dailymean['Month'])['SST'].mean()\n",
    "sst_clim['NO3-DOY'] = sst_dailymean.groupby(sst_dailymean['DOY'])['NO3'].mean()\n",
    "#sst_clim['NO3-Month'] = sst_dailymean.groupby(sst_dailymean['Month'])['NO3'].mean()   \n",
    "\n",
    "# ----- climatology removed in daily T data to get anomalies\n",
    "\n",
    "for ind in sst_dailymean.index:\n",
    "    c = sst_dailymean.loc[ind,'DOY']\n",
    "    \n",
    "    for i in sst_clim.index:        \n",
    "        try:\n",
    "            if i == c:\n",
    "                sst_dailymean.loc[ind,'SST-clim'] = sst_dailymean.loc[ind,'SST'] - sst_clim.loc[i,'SST-DOY']\n",
    "                sst_dailymean.loc[ind,'NO3-clim'] = sst_dailymean.loc[ind,'NO3'] - sst_clim.loc[i,'NO3-DOY']\n",
    "        except IndexError:\n",
    "            continue\n",
    "            \n",
    "# ----- calculate standardized daily anomalies\n",
    "sst_dailymean['SST-clim standardized'] = sp.stats.zscore(sst_dailymean['SST-clim'])\n",
    "sst_dailymean['NO3-clim standardized'] = sp.stats.zscore(sst_dailymean['NO3-clim'])\n",
    "\n",
    "# save daily data\n",
    "sst_dailymean.to_csv(f'/Volumes/Mere/Python_MESMA/Statistics/Environmental_data/AVHRR_SST/AVHRR_daily_summary.csv')\n",
    "        \n",
    "# ----- marine heatwave calculation\n",
    "# definition of a marine heatwave: \n",
    "# temperature greater than 90th percentile based on a 30 year record/baseline (1985 - 2015) = 14.1 degC\n",
    "\n",
    "sst_anom_MHW = sst_dailymean['SST-clim'].quantile(.9)\n",
    "sst_MHW = sst_dailymean['SST'].quantile(.9)\n",
    "\n",
    "# the .diff() function takes the difference between some value and the previous value (I only want values = 5.)\n",
    "MHW_find = pd.DataFrame(sst_dailymean[sst_dailymean['SST'] >= sst_MHW])\n",
    "MHW_find['sequential MHW Days'] = MHW_find.DOY.diff(periods=5)\n",
    "MHW_days = MHW_find[MHW_find['sequential MHW Days'] == 5.0]\n",
    " \n",
    "# ----- create new pandas array grouped by year, with a column for mean, sd. for SST and NO3\n",
    "stats = pd.DataFrame()\n",
    "\n",
    "# SST stuff\n",
    "stats['SST-clim stand Mean'] = sst_dailymean.groupby('Year')['SST-clim standardized'].mean()\n",
    "stats['SST-clim stand Median'] = sst_dailymean.groupby('Year')['SST-clim standardized'].median()\n",
    "stats['SST-clim stand SD'] = sst_dailymean.groupby('Year')['SST-clim standardized'].std()\n",
    "stats['SST-clim stand min'] = sst_dailymean.groupby('Year')['SST-clim standardized'].min()\n",
    "stats['SST-clim stand max'] = sst_dailymean.groupby('Year')['SST-clim standardized'].max()\n",
    "stats['SST-clim stand summer'] = sst_dailymean.groupby(sst_dailymean['Year'][(sst_dailymean['DOY']>=172) & (sst_dailymean['DOY']<=265)])['SST-clim standardized'].mean()\n",
    "stats['SST-clim stand spring'] = sst_dailymean.groupby(sst_dailymean['Year'][(sst_dailymean['DOY']>=78) & (sst_dailymean['DOY']<172)])['SST-clim standardized'].mean()\n",
    "stats['Degree Days'] = sst_dailymean[sst_dailymean['SST'] >= 14.].groupby('Year')['SST'].sum()\n",
    "stats['MHW Days'] = MHW_days['Year'].value_counts()\n",
    "stats = stats.fillna(0) #fill in the nan values with zeros\n",
    "stats['Degree Days-clim'] = stats['Degree Days'] - stats['Degree Days'].mean()\n",
    "stats['Degree Days-clim stand'] = sp.stats.zscore(stats['Degree Days-clim'])\n",
    "stats['MHW Days-clim'] = stats['MHW Days'] - stats['MHW Days'].mean()\n",
    "stats['MHW Days-clim stand'] =  sp.stats.zscore(stats['MHW Days-clim'])\n",
    "\n",
    "# NO3 stuff\n",
    "stats['NO3-clim stand Mean'] = sst_dailymean.groupby('Year')['NO3-clim standardized'].mean()\n",
    "stats['NO3-clim stand Median'] = sst_dailymean.groupby('Year')['NO3-clim standardized'].median()\n",
    "stats['NO3-clim stand SD'] = sst_dailymean.groupby('Year')['NO3-clim standardized'].std()\n",
    "stats['NO3-clim stand min'] = sst_dailymean.groupby('Year')['NO3-clim standardized'].min()\n",
    "stats['NO3-clim stand max'] = sst_dailymean.groupby('Year')['NO3-clim standardized'].max()\n",
    "stats['NO3-clim stand summer'] = sst_dailymean.groupby(sst_dailymean['Year'][(sst_dailymean['DOY']>=172) & (sst_dailymean['DOY']<=265)])['NO3-clim standardized'].mean()\n",
    "stats['NO3-clim stand spring'] = sst_dailymean.groupby(sst_dailymean['Year'][(sst_dailymean['DOY']>=78) & (sst_dailymean['DOY']<172)])['NO3-clim standardized'].mean()\n",
    "\n",
    "stats['Year'] = stats.index\n",
    "\n",
    "# ----- plot SST indices\n",
    "fig,(ax1,ax2,ax3,ax4,ax5,ax6) = plt.subplots(6,1,figsize=(10,15))\n",
    "ax1.bar(stats.index,stats['Degree Days-clim stand'])\n",
    "ax1.set(ylabel = 'Degree Days Index',\n",
    "       xlabel = 'Year',\n",
    "       ylim = [-3.5,3.5])\n",
    "\n",
    "ax2.bar(stats.index,stats['MHW Days-clim stand'])\n",
    "ax2.set(ylabel = 'MHW Days Index',\n",
    "       xlabel = 'Year',\n",
    "       ylim = [-3.5,3.5])\n",
    "\n",
    "ax3.bar(stats.index,stats['SST-clim stand Mean'])\n",
    "ax3.set(ylabel = 'Mean SST Index',\n",
    "       xlabel = 'Year',\n",
    "       ylim = [-3,3])\n",
    "\n",
    "ax4.bar(stats.index,stats['SST-clim stand Median'])\n",
    "ax4.set(ylabel = 'Median SST Index',\n",
    "       xlabel = 'Year',\n",
    "       ylim = [-3,3])\n",
    "\n",
    "ax5.bar(stats.index,stats['SST-clim stand summer'])\n",
    "ax5.set(ylabel = 'Summer SST Index',\n",
    "       xlabel = 'Year',\n",
    "       ylim = [-3,3])\n",
    "\n",
    "\n",
    "ax6.bar(stats.index,stats['SST-clim stand spring'])\n",
    "ax6.set(ylabel = 'Spring SST Index',\n",
    "       xlabel = 'Year',\n",
    "       ylim = [-3,3])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ----- plot NO3 indices\n",
    "fig,(ax1,ax2,ax3,ax4) = plt.subplots(4,1,figsize=(10,12))\n",
    "\n",
    "ax1.bar(stats.index,stats['NO3-clim stand Mean'])\n",
    "ax1.set(ylabel = 'Mean NO3 Index',\n",
    "       xlabel = 'Year',\n",
    "       ylim = [-2,2])\n",
    "\n",
    "ax2.bar(stats.index,stats['NO3-clim stand Median'])\n",
    "ax2.set(ylabel = 'Median NO3 Index',\n",
    "       xlabel = 'Year',\n",
    "       ylim = [-2,2])\n",
    "\n",
    "ax3.bar(stats.index,stats['NO3-clim stand summer'])\n",
    "ax3.set(ylabel = 'Summer NO3 Index',\n",
    "       xlabel = 'Year',\n",
    "       ylim = [-2,2])\n",
    "\n",
    "\n",
    "ax4.bar(stats.index,stats['NO3-clim stand spring'])\n",
    "ax4.set(ylabel = 'Spring NO3 Index',\n",
    "       xlabel = 'Year',\n",
    "       ylim = [-2,2])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# save SST data\n",
    "save_name = ['DegDays','MHWDays','Mean','Median','Summer','Spring']\n",
    "col_name = ['Degree Days-clim stand','MHW Days-clim stand','SST-clim stand Mean','SST-clim stand Median','SST-clim stand summer','SST-clim stand spring']\n",
    "for n,name in enumerate(save_name):\n",
    "    np.savetxt(f'T_{name}_{var}_stand.txt',\n",
    "              stats[['Year',col_name[n]]],\n",
    "              fmt = '%i %0.3f',\n",
    "              header = f'{var}\\n{col_name[n]} \\nrun on:{now}\\nYear; index value')\n",
    "\n",
    "# save NO3 data\n",
    "save_name = ['Mean','Median','Summer','Spring']\n",
    "col_name = ['NO3-clim stand Mean','NO3-clim stand Median','NO3-clim stand summer','NO3-clim stand spring']\n",
    "for n,name in enumerate(save_name):\n",
    "    np.savetxt(f'NO3_{name}_{var}_stand.txt',\n",
    "              stats[['Year',col_name[n]]],\n",
    "              fmt = '%i %0.3f',\n",
    "              header = f'{var}\\n{col_name[n]} \\nrun on:{now}\\nYear; index value')\n",
    "    \n",
    "    \n",
    "# ----- visualize SST data with a CDF\n",
    "sst_dailymean['SST-clim'].hist(cumulative=True, density=1,bins=200)\n",
    "sst_dailymean['SST'].hist(cumulative=True, density=1,bins=200)\n",
    "\n",
    "\n",
    "# ----- plot histograms of T distribution per year\n",
    "sst_dailymean['SST-clim standardized'].hist(figsize = (20,20),by = sst_dailymean['Year'], sharey = True,sharex = True, bins=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Process NOAA buoy data: remove climatology NOAA NBDC - Bodega Bay Station 46013\n",
    "filename = 'NBDC_BodegaBay_all.csv'\n",
    "all_data = pd.read_csv(filename)\n",
    "all_data = all_data.replace(999,np.NaN)\n",
    "\n",
    "# get daily data\n",
    "\n",
    "daily_data = all_data.groupby(['DATE_ID']).mean()\n",
    "daily_data.index = pd.to_datetime(daily_data.index,format='%Y%m%d')\n",
    "\n",
    "daily_data.insert(1,'DOY', 0)\n",
    "daily_data['DOY'] = daily_data.index.dayofyear\n",
    "daily_data = daily_data.drop('hh',1).drop('mm',1)\n",
    "#daily_data = daily_data.drop('MM',1).drop('DD',1).drop('hh',1).drop('mm',1)\n",
    "\n",
    "\n",
    "# add climatology and anomaly columns to daily data\n",
    "\n",
    "anom_data = pd.DataFrame(index=daily_data.index,columns=daily_data.keys(),dtype=float)\n",
    "anom_data['DOY'] = daily_data['DOY']\n",
    "anom_data['YY'] = daily_data['YY']\n",
    "anom_data['MM'] = daily_data['MM']\n",
    "anom_data['DD'] = daily_data['DD']\n",
    "\n",
    "\n",
    "\n",
    "# daily climatology across the timeseries\n",
    "\n",
    "NOAA_climatology = daily_data.groupby(['DOY']).mean()\n",
    "\n",
    "\n",
    "# remove daily climatology from daily data\n",
    "\n",
    "for c in NOAA_climatology.index:\n",
    "    for i in daily_data.index:\n",
    "        val = daily_data.at[i,'DOY']\n",
    "        try:\n",
    "            if val == c:\n",
    "                anom_data.loc[i,'WD':'PRES'] = daily_data.loc[i,'WD':'PRES'].subtract(NOAA_climatology.iloc[c,1:])\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "all_data.to_csv('NBDC_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Process NOAA buoy data: condense relevant Hs and SST from NOAA NBDC - Bodega Bay Station 46013\n",
    "\n",
    "var = 'NBDC'\n",
    "\n",
    "# ----- create timeseries with just T, Hs\n",
    "\n",
    "T_Hs_anom_data = pd.DataFrame(index=anom_data.index,columns=['YY','MM','DOY','WVHT','WTMP'])\n",
    "T_Hs_anom_data['YY'] = anom_data['YY']\n",
    "T_Hs_anom_data['MM'] = anom_data['MM']\n",
    "T_Hs_anom_data['DOY'] = anom_data['DOY']\n",
    "T_Hs_anom_data['WVHT'] = anom_data['WVHT']\n",
    "T_Hs_anom_data['WTMP'] = anom_data['WTMP']\n",
    "     \n",
    "# ----- creating annual anomaly indices for all variables\n",
    "# max annual wave height and temp anomolies, max wave height and temp anomolies timing\n",
    "\n",
    "annual_anom = pd.DataFrame(index=np.arange(T_Hs_anom_data['YY'].min(),T_Hs_anom_data['YY'].max()+2,1),columns=['WVHT','WTMP'])\n",
    "monthly_anom = anom_data.groupby(['MM']).mean()\n",
    "\n",
    "\n",
    "for year in annual_anom.index:\n",
    "    for key in T_Hs_anom_data.keys()[2:]:\n",
    "        annual_anom_max = T_Hs_anom_data[T_Hs_anom_data['YY'] == year][key].max()\n",
    "        annual_array = np.array((T_Hs_anom_data[T_Hs_anom_data['YY'] == year]['DOY'],T_Hs_anom_data[T_Hs_anom_data['YY'] == year][key])).T\n",
    "        try:\n",
    "            index_max = np.argmax(annual_array,axis=0)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        annual_doy_max = annual_array[index_max[1],0]\n",
    "        annual_anom_min = anom_data[anom_data['YY'] == year][key].min()\n",
    "        index_min =  np.argmin(annual_array,axis=0)      \n",
    "        annual_doy_min = annual_array[index_min[1],0]\n",
    "      \n",
    "        if abs(annual_anom_min)>annual_anom_max:\n",
    "            annual_anom.loc[year,key] = annual_anom_min\n",
    "            #annual_DOY.loc[year,key] = annual_doy_min\n",
    "            #annual_array.loc[year,key] = annual_doy_min\n",
    "        else:\n",
    "            annual_anom.loc[year,key] = annual_anom_max\n",
    "            #annual_DOY.loc[year,key] = annual_doy_max\n",
    "            #annual_array.loc[year,key] = annual_doy_max\n",
    "\n",
    "\n",
    "# rename and add columns to annual_anom\n",
    "\n",
    "annual_anom = annual_anom.rename(columns = {'WVHT':'WVHT_max','WTMP':'WTMP_max'})\n",
    "add_cols = ['WVHT_mean','WTMP_phys','WTMP_mean','WTMP_spring','WTMP_summer','WVHT_winter']\n",
    "\n",
    "for i in range(2,6):\n",
    "        annual_anom.insert(i,add_cols[i-3],0)\n",
    "\n",
    "# number of days SST >17 degC\n",
    "\n",
    "SST_phys_calc = daily_data.loc[daily_data['WTMP']>17.]\n",
    "SST_phys_counts = SST_phys_calc['YY'].value_counts()\n",
    "for year in annual_anom.index:\n",
    "    try:\n",
    "        annual_anom['WTMP_phys'].loc[year] = SST_phys_counts.loc[year]\n",
    "    except KeyError:\n",
    "        continue\n",
    "        \n",
    "# Avg SST anomaly during sporophyte growth season (spring; Mar 20 - June 21)\n",
    "# Avg SST anomaly during max photosyn season ( summer; June 22 - Sept 23)\n",
    "# physiological T limits of nereo - 3 to 17 degrees C\n",
    "# min T across this timeseries is ~8 deg C\n",
    "\n",
    "spring_all = pd.DataFrame(anom_data.loc[(anom_data['DOY']<=172) & (anom_data['DOY']>=80)],dtype=float)\n",
    "summer_all = pd.DataFrame(anom_data.loc[(anom_data['DOY']>=173) & (anom_data['DOY']<=266)],dtype=float)\n",
    "winter_all = pd.DataFrame(anom_data.loc[anom_data['DOY']<=79],dtype=float)\n",
    "\n",
    "annual_anom['WTMP_spring'] = spring_all.groupby(['YY'])['WTMP'].mean()\n",
    "annual_anom['WTMP_summer'] = summer_all.groupby(['YY'])['WTMP'].mean()\n",
    "annual_anom['WTMP_mean'] = anom_data.groupby(['YY'])['WTMP'].mean()\n",
    "\n",
    "annual_anom['WVHT_winter'] = winter_all.groupby(['YY'])['WVHT'].mean()\n",
    "annual_anom['WVHT_mean'] = anom_data.groupby(['YY'])['WVHT'].mean()\n",
    "\n",
    "\n",
    "# ----- Index stadardization \n",
    "# because there are NaNs in the timeseries I have to calculate zscore by hand rather than with the scipy function\n",
    "\n",
    "T_max_LS =  pd.DataFrame(annual_anom.loc[annual_anom.index >= 1985,'WTMP_max'])\n",
    "T_max_LS['cubic interp'] = T_max_LS['WTMP_max'].interpolate(method='cubic',limit=None)\n",
    "T_max_stand = np.array((T_max_LS.index,(T_max_LS['WTMP_max'] - T_max_LS['WTMP_max'].mean())/T_max_LS['WTMP_max'].std(ddof=0))).T\n",
    "T_max_stand_interp = np.array((T_max_LS.index,(T_max_LS['cubic interp'] - T_max_LS['cubic interp'].mean())/T_max_LS['cubic interp'].std(ddof=0))).T\n",
    "\n",
    "T_phys_LS = pd.DataFrame(annual_anom.loc[annual_anom.index >= 1985,'WTMP_phys'])\n",
    "T_phys_stand = np.array((T_phys_LS.index,(T_phys_LS['WTMP_phys'] - T_phys_LS['WTMP_phys'].mean())/T_phys_LS['WTMP_phys'].std(ddof=0))).T\n",
    "\n",
    "T_mean_LS = pd.DataFrame(annual_anom.loc[annual_anom.index >= 1985,'WTMP_mean'])\n",
    "T_mean_stand = np.array((T_mean_LS.index,(T_mean_LS['WTMP_mean'] - T_mean_LS['WTMP_mean'].mean())/T_mean_LS['WTMP_mean'].std(ddof=0))).T\n",
    "\n",
    "T_spring_LS = pd.DataFrame(annual_anom.loc[annual_anom.index>=1985,'WTMP_spring'])\n",
    "T_spring_LS['cubic interp'] = T_spring_LS['WTMP_spring'].interpolate(method='cubic',limit=None)\n",
    "T_spring_LS = pd.DataFrame(T_spring_LS.loc[T_spring_LS.index>=1985])\n",
    "T_spring_stand = np.array((T_spring_LS.index,(T_spring_LS['WTMP_spring'] - T_spring_LS['WTMP_spring'].mean())/T_spring_LS['WTMP_spring'].std(ddof=0))).T\n",
    "T_spring_stand_interp = np.array((T_spring_LS.index,(T_spring_LS['cubic interp'] - T_spring_LS['cubic interp'].mean())/T_spring_LS['cubic interp'].std(ddof=0))).T\n",
    "\n",
    "T_summer_LS = pd.DataFrame(annual_anom.loc[annual_anom.index>=1985,'WTMP_summer'])\n",
    "T_summer_LS['cubic interp'] = T_summer_LS['WTMP_summer'].interpolate(method='cubic',limit=None)\n",
    "T_summer_stand = np.array((T_summer_LS.index,(T_summer_LS['WTMP_summer'] - T_summer_LS['WTMP_summer'].mean())/T_summer_LS['WTMP_summer'].std(ddof=0))).T\n",
    "T_summer_stand_interp = np.array((T_summer_LS.index,(T_summer_LS['cubic interp'] - T_summer_LS['cubic interp'].mean())/T_summer_LS['cubic interp'].std(ddof=0))).T\n",
    "\n",
    "HS_max_LS = pd.DataFrame(annual_anom.loc[annual_anom.index>=1985,'WVHT_max'])\n",
    "HS_max_stand = np.array((HS_max_LS.index,(HS_max_LS['WVHT_max'] - HS_max_LS['WVHT_max'].mean())/HS_max_LS['WVHT_max'].std(ddof=0))).T\n",
    "\n",
    "HS_mean_LS = pd.DataFrame(annual_anom.loc[annual_anom.index>=1985,'WVHT_mean'])\n",
    "HS_mean_stand = np.array((HS_mean_LS.index,(HS_mean_LS['WVHT_mean'] - HS_mean_LS['WVHT_mean'].mean())/HS_mean_LS['WVHT_mean'].std(ddof=0))).T\n",
    "\n",
    "HS_winter_LS = pd.DataFrame(annual_anom.loc[annual_anom.index>=1985,'WVHT_winter'])\n",
    "HS_winter_LS['cubic interp'] = HS_winter_LS['WVHT_winter'].interpolate(method='cubic',limit=None)\n",
    "HS_winter_stand = np.array((HS_winter_LS.index,(HS_winter_LS['WVHT_winter'] - HS_winter_LS['WVHT_winter'].mean())/HS_winter_LS['WVHT_winter'].std(ddof=0))).T\n",
    "HS_winter_stand_interp = np.array((HS_winter_LS.index,(HS_winter_LS['cubic interp'] - HS_winter_LS['cubic interp'].mean())/HS_winter_LS['cubic interp'].std(ddof=0))).T\n",
    "\n",
    "\n",
    "# ----- Output of buoy data to text files\n",
    "\n",
    "NBDC_buoy = 'Bodega Bay'\n",
    "\n",
    "var_names = [HS_mean_stand, HS_max_stand,HS_winter_stand,T_max_stand,T_phys_stand,T_mean_stand,\n",
    "             T_spring_stand,T_summer_stand]\n",
    "\n",
    "var_str = ['HS_mean_stand','HS_max_stand','HS_winter_stand','T_max_stand','T_phys_stand','T_mean_stand',\n",
    "           'T_spring_stand','T_summer_stand']\n",
    "\n",
    "var_descript = ['annual mean Hs anomaly - climatology removed and index standardized',\n",
    "                'annual max Hs anomaly - climatology removed and index standardized',\n",
    "                'annual winter mean Hs anomaly - climatology removed and index standardized',\n",
    "                'annual max T anomaly - climatology removed and index standardized',\n",
    "                'annual number of days greater than 17 degC - climatology not removed and index standardized',\n",
    "                'annual mean of temperature anomalies - climatology removed and index standardized',\n",
    "                'annual spring mean T anomaly - climatology removed and index standardized',\n",
    "                'annual summer mean T anomaly - climatology removed and index standardized']\n",
    "                \n",
    "\n",
    "for n,name in enumerate(var_names):\n",
    "\n",
    "    np.savetxt(f'{var}_{var_str[n]}.txt',\n",
    "              name,fmt = '%i %0.5f',\n",
    "               header = f'{var} {NBDC_buoy} \\n{var_descript[n]} \\nrun on:{now}\\nYear; index value')\n",
    "    \n",
    "\n",
    "# ----- Output of interpolated buoy data to text files\n",
    "var_names_interp = [HS_winter_stand_interp,T_max_stand_interp,T_spring_stand_interp,T_summer_stand_interp]\n",
    "\n",
    "var_str_interp = ['HS_winter_stand_interp','T_max_stand_interp','T_spring_stand_interp','T_summer_stand_interp']\n",
    "\n",
    "var_descript_interp = ['annual winter mean Hs anomaly - climatology removed, interpolated over data gaps, and index standardized',\n",
    "               'annual max T anomaly - climatology removed, interpolated over data gaps, and index standardized',\n",
    "               'annual spring mean T anomaly - climatology removed, interpolated over data gaps, and index standardized',\n",
    "               'annual summer mean T anomaly - climatology removed, interpolated over data gaps, and index standardized']\n",
    "\n",
    "for n,name in enumerate(var_names_interp):\n",
    "\n",
    "    np.savetxt(f'{var}_{var_str_interp[n]}.txt',\n",
    "              name,fmt = '%i %0.5f',\n",
    "               header = f'{var} {NBDC_buoy} \\n{var_descript_interp[n]} \\nrun on:{now}\\nYear; index value')\n",
    "         \n",
    "anom_data.to_csv('NBDC_BodegaBay_anom.csv',\n",
    "             na_rep=999,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biological data\n",
    "# Reef Check Purple urchin\n",
    "# Reef Check Pycnopodia\n",
    "\n",
    "\n",
    "var = 'bio'\n",
    "\n",
    "filelist = glob.glob('CDFW*_means.xlsx')\n",
    "biology = pd.DataFrame()\n",
    "\n",
    "for file in filelist:\n",
    "    try:\n",
    "        data = pd.read_excel(file,'means')\n",
    "        biology = biology.append(data)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "#purps_mean = biology['CDFW+RC Purps Annual Mean'].mean()\n",
    "#pycno_mean = biology['CDFW+RC Pycno Annual Mean'].mean()\n",
    "means = biology.iloc[biology.index<11].mean()\n",
    "\n",
    "# calculate anomolies - no climatology removed (need to look up how the data is collected and processed)\n",
    "bioanoms = pd.DataFrame(columns = ['Year','Purple Urchin Anom','Purple Urchin Stand Index','Pycnopodia Anom','Pycnopodia Stand Index'])\n",
    "bioanoms['Year'] = biology['Year']\n",
    "#bioanoms['Purple Urchin Anom'] = biology['CDFW+RC Purps Annual Mean'] - purps_mean\n",
    "bioanoms['Purple Urchin Anom'] = biology['CDFW+RC Purps Annual Mean'] - means['CDFW+RC Purps Annual Mean']\n",
    "#bioanoms['Pycnopodia Anom'] = biology['CDFW+RC Pycno Annual Mean'] - pycno_mean\n",
    "bioanoms['Pycnopodia Anom'] = biology['CDFW+RC Pycno Annual Mean'] - means['CDFW+RC Pycno Annual Mean']\n",
    "bioanoms['Purple Urchin Stand Index'] = sp.stats.zscore(bioanoms['Purple Urchin Anom'],nan_policy='omit')\n",
    "bioanoms['Pycnopodia Stand Index'] = sp.stats.zscore(bioanoms['Pycnopodia Anom'],nan_policy='omit')\n",
    "\n",
    "save_name = ['bio_purps','bio_pycno']\n",
    "col_name = ['Purple Urchin Stand Index','Pycnopodia Stand Index']\n",
    "for n,name in enumerate(save_name):\n",
    "    np.savetxt(f'{var}_{name}_index_stand.txt',\n",
    "              bioanoms[['Year',col_name[n]]],\n",
    "              fmt = '%i %0.3f',\n",
    "              header = f'{var} - {col_name[n]}: Indices are in density (units of # per 60 m2) \\nrun on:{now}\\nYear; index value')\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "#ax.bar(bioanoms['Year'],bioanoms['Purple Urchin Anom'],label='Purple Urchins',color='purple')\n",
    "ax.bar(bioanoms['Year'],bioanoms['Purple Urchin Stand Index'],label='Purple Urchins',color='purple')\n",
    "ax.bar(bioanoms['Year'],bioanoms['Pycnopodia Stand Index'],label='Pycnopodia',color='grey')\n",
    "#ax.bar(bioanoms['Year'],bioanoms['Pycnopodia Anom'],label='Pycnopodia',color='grey')\n",
    "ax.set(ylabel = 'Biological Index',\n",
    "      xlabel = 'Year',\n",
    "      ylim = [-2,3])\n",
    "fig.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeseries plot of largescale indices\n",
    "\n",
    "fig1,(ax1,ax2,ax3,ax4,ax5,ax6,ax7) = plt.subplots(7,1,figsize = (9,15),sharex = True)\n",
    "ax1.bar(kelpanom.index,kelpanom['Standardized Kelp anom'],color='lightsteelblue')\n",
    "ax1.set(ylim = [-3,3])\n",
    "ax1.set_title('Sonoma County Bull Kelp Index and Relevant Environmental Variables', fontsize = 18)\n",
    "ax1.set_ylabel('Bull kelp\\n(summed frac)', fontsize=12)\n",
    "\n",
    "ax2.bar(PDO_stand[:,0],PDO_stand[:,1],color='lightsteelblue')\n",
    "ax2.set(ylim = [-2,2])\n",
    "ax2.set_ylabel('PDO Index',fontsize = 12)\n",
    "\n",
    "ax3.bar(NPGO_stand[:,0],NPGO_stand[:,1],color='lightsteelblue')\n",
    "ax3.set(ylim = [-2,2])\n",
    "ax3.set_ylabel('NPGO Index',fontsize = 12)\n",
    "\n",
    "ax4.bar(MEI_stand[:,0],MEI_stand[:,1],color='lightsteelblue')\n",
    "ax4.set(ylim = [-2,2])\n",
    "ax4.set_ylabel('MEI Index',fontsize = 12)\n",
    "\n",
    "ax5.bar(ONI_stand[:,0],ONI_stand[:,1],color='lightsteelblue')\n",
    "ax5.set(ylim = [-2,2])\n",
    "ax5.set_ylabel('ONI Index',fontsize = 12)\n",
    "\n",
    "ax6.bar(near_max_stand[:,0],near_max_stand[:,1],color='lightsteelblue')\n",
    "ax6.set(ylim = [-3,3])\n",
    "ax6.set_ylabel('Nearshore ssha \\n(mm)',fontsize = 12)\n",
    "\n",
    "ax7.bar(off_max_stand[:,0],off_max_stand[:,1],color='lightsteelblue')\n",
    "ax7.set(xlim = [1985,2019],\n",
    "       ylim = [-3,3])\n",
    "ax7.set_xlabel(xlabel = 'Year',fontsize = 14)\n",
    "ax7.set_ylabel('Offshore ssha \\n(mm)',fontsize = 12)\n",
    "\n",
    "fig1.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
